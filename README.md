ğŸ“š LLM-Powered RAG Document Question Answering System

A production-style Retrieval-Augmented Generation (RAG) system that answers questions from user-uploaded documents with explainability, evaluation metrics, failure analysis, latency & cost monitoring.

This project focuses not just on getting answers, but on understanding why an answer was generated or refused â€” a critical requirement for real-world LLM systems.

ğŸš€ Key Highlights

âœ… End-to-end RAG pipeline (Retrieval â†’ Filtering â†’ Generation)

ğŸ§  Explainability dashboard (used vs ignored context)

ğŸ“Š Evaluation metrics (recall@k, coverage, faithfulness, grounding)

âŒ Failure-case analysis (why the model refused to answer)

â±ï¸ Latency breakdown (retrieval vs LLM)

ğŸ’° Cost estimation (token usage & estimated cost)

ğŸ§© Modular, extensible architecture

ğŸŒ Local + public demo support (ngrok)

ğŸ³ Docker-ready (explored for cloud deployment)

ğŸ—ï¸ System Architecture
User Query
   â†“
Query Rewriting
   â†“
Vector Retrieval (Top-K)
   â†“
Similarity Filtering (Threshold)
   â†“
Context Selection
   â†“
LLM Answer Generation
   â†“
Evaluation + Explainability + Metrics

ğŸ§  Core Concepts Implemented
ğŸ”¹ Retrieval-Augmented Generation (RAG)

Prevents hallucination by grounding answers in retrieved document chunks

Uses similarity-based filtering to control relevance

ğŸ”¹ Explainability (Why this answer?)

Shows:

Which chunks influenced the answer

Which chunks were retrieved but ignored

Why certain context was rejected

ğŸ”¹ Failure-Case Dashboard

When no answer is generated, the system explains:

Similarity threshold violation

Highest retrieved score

Concrete steps to fix the issue (lower threshold, increase Top-K, add documents)

ğŸ”¹ Evaluation Metrics

Recall@K â€“ retrieval quality

Context Coverage â€“ how much of the answer is grounded

Faithfulness â€“ consistency with retrieved context

Grounding Score â€“ hallucination risk indicator

ğŸ”¹ Performance Monitoring

Retrieval latency

LLM latency

Total request latency

Token usage & estimated cost

ğŸ—‚ï¸ Project Structure
LLM-RAG-Document-QA/
â”‚
â”œâ”€â”€ app.py                  # FastAPI backend (API version)
â”œâ”€â”€ streamlit_app.py        # Streamlit UI (direct pipeline version)
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ rag_service.py      # Core RAG orchestration
â”‚   â”œâ”€â”€ state.py            # Global state & embeddings
â”‚   â””â”€â”€ schemas.py          # Request / response schemas
â”‚
â”œâ”€â”€ rag_core/
â”‚   â””â”€â”€ pipeline.py         # RAG pipeline abstraction
â”‚
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ similarity.py       # Vector similarity retrieval
â”‚
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ llm_model.py        # LLM wrapper
â”‚   â””â”€â”€ utils.py            # Token estimation
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ retrieval_metrics.py
â”‚   â”œâ”€â”€ context_coverage.py
â”‚   â”œâ”€â”€ faithfulness.py
â”‚   â””â”€â”€ hallucination.py
â”‚
â”œâ”€â”€ data/                   # Uploaded documents
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile              # (Explored for deployment)
â””â”€â”€ README.md

ğŸ–¥ï¸ Running Locally
1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

2ï¸âƒ£ Run the Streamlit app
streamlit run streamlit_app.py

3ï¸âƒ£ Open in browser
http://localhost:7860

ğŸŒ Public Demo (Optional)

The app was successfully exposed using ngrok for mobile and external access:

ngrok http 7860


This generates a public HTTPS URL usable on any device.

ğŸ³ Deployment Notes (Important)

Docker-based deployment was explored (Hugging Face Spaces)

Due to:

heavy initialization

embedding state

RAG pipeline startup costs

Hugging Face Spaces showed intermittent runtime issues

â¡ï¸ This is a platform limitation, not an architectural flaw.

In real-world setups, this system is better suited for:

AWS EC2 / ECS

Azure App Service

GCP Cloud Run

ğŸ¯ Why This Project Matters

This project goes beyond toy RAG demos by addressing real production concerns:

Explainability (trust)

Failure analysis (debuggability)

Cost awareness (scalability)

Performance monitoring (latency)

These are the exact concerns evaluated in:

ML engineer interviews

Applied AI roles

Startup MVP discussions

ğŸ”® Future Extensions

Multimodal RAG (PDF + images)

Hybrid retrieval (BM25 + vectors)

Query intent classification

RAG evaluation automation

Agent-based document workflows

Cloud-native deployment (AWS/GCP)

ğŸ‘¤ Author

Anshu Pandey
Aspiring Machine Learning & AI Engineer
Focused on building scalable, explainable ML systems
